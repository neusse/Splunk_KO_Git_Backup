{
	"args": "constraint",
	"author": "admin",
	"disabled": "0",
	"eai:acl.app": "trackme",
	"eai:acl.can_change_perms": "1",
	"eai:acl.can_list": "1",
	"eai:acl.can_share_app": "1",
	"eai:acl.can_share_global": "1",
	"eai:acl.can_share_user": "0",
	"eai:acl.can_write": "1",
	"eai:acl.modifiable": "1",
	"eai:acl.owner": "admin",
	"eai:acl.perms.read": "*",
	"eai:acl.perms.write": "['admin', 'trackme_admin']",
	"eai:acl.removable": "0",
	"eai:acl.sharing": "global",
	"eai:appName": "trackme",
	"eai:userName": "nobody",
	"iseval": "0",
	"published": "",
	"splunk_server": "TheNeusse",
	"target": "admin/macros",
	"title": "trackme_data_sampling_return_live_sample(1)",
	"updated": "1969-12-31T16:00:00-08:00",
	"definition": 
savedsearch runSPL [

| inputlookup trackme_data_source_monitoring where (_key="$constraint$" OR data_name="$constraint$") | eval key=_key
`comment("##### Once the KVstore content is loaded, we need to adress the specific case of Elastic sources #####")`
| `trackme_lookup_elastic_sources`

`comment("##### Define the search contraint dynamically #####")`
| eval search_constraint = if(isnull(elastic_source_search_constraint), "index=\"" . data_index . "\" " . "sourcetype=\"" . data_sourcetype . "\"", elastic_source_search_constraint)
| fields key, data_name, data_sourcetype, search_constraint, elastic_source_search_constraint, elastic_source_search_mode | where NOT (elastic_source_search_mode="mstats" OR elastic_source_search_mode="from")

`comment("##### Specific to cribl integration #####")`
| rex field=data_name "\|cribl:(?<cribl_pipe>.*)"
| eval search_constraint = if(isnotnull(cribl_pipe), search_constraint . " cribl_pipe::" . cribl_pipe, search_constraint)

`comment("##### Lookup the data sampling collection to retrieve the iteration ID of the data sampling, we use this to conditionally define the number of events to be retrieved at the first iteration #####")`
`comment("##### In addition filter on entries where the feature is enabled or new entries only")`
| lookup trackme_data_sampling _key as key OUTPUT data_sample_iteration, data_sample_feature, data_sampling_nr, data_sample_anomaly_ack_status
| fillnull value="enabled" data_sample_feature
| eval data_sample_iteration=if(isnull(data_sample_iteration), 0, data_sample_iteration)

`comment("##### Finally generate the search to be used, the number of events sample to retrieve depends if this is the iteration for this entity #####")`

`comment("##### If a number of records to sample is configured, this value will be used in priority, otherwise we define these via macro and conditions #####")`
| eval events_sample_range=if(isnum(data_sampling_nr), data_sampling_nr, if(data_sample_iteration=0, `trackme_data_sampling_default_sample_record_at_discovery`, `trackme_data_sampling_default_sample_record_at_run`) )

| eval spl=search_constraint . " | head " . events_sample_range . " | eval key = \"" . key . "\" | eval data_name = \"" . data_name . "\" | stats values(_raw) as raw_sample by key, data_name | eval data_sourcetype = \"" . data_sourcetype . "\" | mvexpand raw_sample"
| fields spl

| eval prefix="| append [ search "
| eval suffix=" ]"

| streamstats count as line_count
| eval spl = if(line_count!=1, prefix . spl . suffix, spl)

| fields spl

| mvcombine delim=" " spl | nomv spl

| append [ | makeresults | eval spl=if(isnull(spl), "| makeresults", spl) | fields - _time ] | head 1

]

| fields raw_sample | mvexpand raw_sample
}
